Speed up repeated TokenStream init
by caching isMethodOverridden results I ve got my test environ all setup so I ll be happy to test me too not a real benchmark but i think average sized docs unrelated to TokenStream init but what appears to be creating additional slowdown is this TRACE 300197 org.apache.lucene.util.AttributeSource.getAttributeImplsIterator AttributeSource.java 140 org.apache.lucene.util.AttributeSource.clearAttributes AttributeSource.java 233 org.apache.lucene.analysis.CharTokenizer.incrementToken CharTokenizer.java 56 org.apache.lucene.index.DocInverterPerField.processFields DocInverterPerField.java 189 even with reusableTokenStreams I am still seeing a slowdown and with fairly granular profiling this is consistently at the top. is there a way to optimize clear in any way Here a poatch. About the naming of the private internal class we can discuss but it seems to work. No real performance tests until now. Could you please test and compare the results with before is there a way to optimize clear in any way The cost is creating the iterator. As it is a LinkedHashMap there must be used an iterator because direct access is slow but for extra safety the iterator is made unmodifable this wrapper around could be removed. New patch that also removes the Collections.unmodifiableXxx calls. If somebody removes elements from the iterator its not our problem ok lemme restart my benchmark... the performance difference i was seeing with your first patch so far looked minor but consistent. here s some unscientific numbers from your first patch indexing hamshari about 500MB corpus 165k docs analyzer with no reusableTS without patch 52988 ms 52902 ms 53035 ms 53116 ms 52637 ms analyzer with no reusableTS with patch 51916 ms 51969 ms 51872 ms 52438 ms 51710 ms Test on the first patch Almost brings things back to par with Yoniks short solr indexing test previous results were like 90 some seconds Before TokenStream revamp iter 100000 time 44173 throughput 2263 iter 100000 time 44403 throughput 2252 After TokenStream revamp iter 100000 time 46720 throughput 2140 iter 100000 time 47038 throughput 2125 That method inspection was like the second hotest method see the profiling results and this must just take it right out of there. By the way I havn t looked if it slows down the single use case at all or not. No noticeable diff in the second patch for me. here are my large doc numbers with the second patch same setup no reusableTS 51347ms 49917ms 50676ms 50010ms 49261ms seems to help a bit. i will turn back on reusableTS and see if the iterator still shows up in profiling. Mark I do not understand your comment completely do you mean with before after revamp the comparison between old Lucene before-Attributes TokenStreams compared to the patched Attribute ones And the 90 seconds is the unpatched Attributes case looks like half speed If so the problem was really this reflection calls. Sorry - to be a bit more clear Lucene trunk without your patch was like 90 seconds - twice as slow as a previous version of Lucene on Yoniks short doc test. The numbers reported above are for Lucene pre the TokenStream improvements and post with your patch - so now the numbers are much closer - though its still a tad slower. You second patch didnt change things for me from your first patch - same perf in this test. If so the problem was really this reflection calls. Yes - which you can clearly see from the profiling results I got yesterday http myhardshadow.com images before.png http myhardshadow.com images after.png isMethodOverriden was called way too often for its speed - its a highly inefficient call if you trace through it. If it is still a little bit slower in my opinion its a little bit 2 seconds of 45s is 4 could it be because the Solr TokenFilters only implement next Token and not incrementToken What is your distribution of TokenStreams Filters in your test. Only new ones only old ones both Yes indeed its very close now. The filters are tokenizer class solr.WhitespaceTokenizerFactory filter class solr.WordDelimiterFilterFactory generateWordParts 1 generateNumberParts 0 catenateWords 0 catenateNumbers 0 catenateAll 0 filter class solr.LowerCaseFilterFactory So I guess its 1 and 1 Lowercase comes from Lucene and implements increment but WordDelim just does next. The other hotspot was though much less of one TokenStream init AttributeSource - if implementing increment relieves that it might be the same speed after. Uwe your patch seems to help my large doc case although as you can see the numbers are still very different if i implement reusableTS than if i do not. With patch reusableTS Total time 43373 Total time 43428 Total time 43536 Total time 42857 Total time 42835 Without patch reusableTS Total time 44613 Total time 45720 Total time 45592 Total time 45445 Total time 45090 Also with your patch the org.apache.lucene.util.AttributeSource.getAttributeImplsIterator from clearAttributes is ranked 8 instead of 1 or 2 in profiling. Mark The only hotspot could be initTokenWrapper which does some checks but normally shortcuts in filters to the input TokenStream s TokenWrapper instance. Using incrementToken would not help here only if you switch of using the old API complete using TokenStream.setOnlyUseNewAPI true which can only be done globally so all Solr TokenStreams must implement incrementToken . Attached is a patch that removes the clearAttributes from CharTokenizer see discussion with Yonik on java-dev and also removes clear calls where not really needed. Uwe removal of the CharTokenizer clearAttributes makes a difference for me compare to my last numbers 38729ms 40201ms 40085ms 40238ms 40169ms Another good cache Uwe AttributeSource.clearAttributes could use the State which is also used for cloning to iterate the AttributeImpls faster. But if you use the State and there is no state already created it would have the cost of capturing the state cloning for no real use... I have another idea Why not make the AttributeImpls itsself a linked list. So each AttributeImpl gets a member called nextAttributeImpl for the linked list. Whenever an AttributeImpl is added to an AttributeSource the last added AttributeImpl s next is set to the added one. By this iterating over AttributeImpls is just a simple for-loop from the first attribute. The AttributeSource must hold a reference to the first and last Attribute but only one-direction linkage from first to last. As Attributes can only added to one AttributeSource and not to multiple ones I see no problem with it. And AttributeImpls can also not removed so no problem at all. You don t have to call captureState and clone. You just need to call computeCurrentState one time to create the internal state. That is basically a linked list. If then someone adds more attributes to the source the state is invalidated and you would have to call computeCurrentState again. Ah you are right I will try this out. The Iterator returned by getAttributeImplsIterator could then also be implemented using the State. This iterator would not be used internally by AttributeSource there the State is iterated directly but e.g. in TeeSinkTokenStream. So supplying an iterator is still needed but internally the faster direct State iteration can be used. I will try this out and re-enable clearAttribute in the Tokenizers again. New patch that optimizes the iteration over the AttributeImpls using the computed State linked list. This also adds the default buffer size to KeywordTokenizer that got lost during the move to the new API. To test performance I reactivated the clearAttributes call in CharTokenizer. If this is now all ok I would like to fix this issue as soon as possible to be able to do more perf testing with the optimized impls. The big hammer of isMethodOverridden is now removed and speed came back to the original one with some small slowdown caused by mixing old and new TokenFilters together . Nice work Uwe The latest patch appears to hurt the Solr use case a bit - went from 46-47 seconds to 51-52 remember its 43-45 with the pre reflection stuff Hm and with the termAtt.clear instead of clearAttributes Was the 46-47 with the clearAttributes call or without You always have the problem with very short TokenStreams that are not reused that the initialization and State linked-list construction needs some time. In the reused case it should be faster with the latest patch or without clearAttributes at all. Where is the hotSpot Do you have a figure like before the patch with method execution times I was getting 46-47 with both of the first two patches. I can double check a little later though. uwe in my case the latest patch performs approx the same as your patch where CharTokenizer clearAttributes was removed avg 40893ms . Thanks. And mine was a misreport - sorry - a wine program was eating one of my 4 cores and I didn t notice - its testing at about 48 s now so just about the same as the first 2 patches. OK. Last patch I only added a test in TestAttributeSource that verifies the correctness of the returned getAttributeImplsIterator important because iterator logic is not so simple to understand... . TestTeeSinkTokenFilter would also fail with wrong Iterator... I think I commit this shortly Any complaints Just to complete my report The tests I reported in this issue were done with a little more beef in the documents - I had added about 4 lines from a newspaper article. The result is that we are only about 4-5 slower using those documents now. However with Yonik s original test with very short docs String fields text simple text test text how now brown cow text what s that text radical text what s all this about anyway text just how fast is this text indexing ...we are 10 behind. This is a mix of TokenStreams - you can see the tokenfilters used in the profile pics. This is huge improvement from before - that was 50-60 slower with this test. All profiling pics are from Yoniks original small doc test with 100000 iterations. I ll attach before the reflection token stream stuff after trunk after with this patch I just want to say I think that 10 test case might be a worst case very short documents and no reusableTS. I ran a bunch of iterations on a corpus regular sized docs though and found this Lucene 2.4.1 CzechAnalyzer does not implement reusableTS avg 48290.4ms HEAD LUCENE-1796 CzechAnalyzer does not implement reusableTS avg 49943.8ms a bit slower HEAD LUCENE-1796 LUCENE-1794 CzechAnalyzer implements reusableTS avg 47846.1ms a bit faster So I think reusableTS in Solr combined with this patch can mitigate any remaining construction overhead and maybe be faster overall compared to the last release. The shorter the text the more the construction cost increases. This is what I exspect. For normal text length like abstracts newspaper articles and so on the speed is equal than before. I think I commit this now and leave all other things to the current discussion on java-dev. Committed revision 802930 I only removed the clearAttributes call again which is unneeded for CharTokenizer I think I commit this now and leave all other things to the current discussion on java-dev. 1. Good work Uwe Thanks. I only removed the clearAttributes call again which is unneeded for CharTokenizer I thought that Tokenizers had to clear all attributes We had no conclusion on this. I think we should create a new issue out of it and change all Tokenizers to clear all Attributes in incrementToken . This requirement then should also be noted in the JavaDocs of Tokenizer. Currently no Tokenizer clears the attributes... AttributeSource.clearAttributes is never called at the moment. I think Token.reset wasn t called before either. So I think we always had this potential problem Token.clear used to be called by the consumer... but then it was switched to the producer here https issues.apache.org jira browse LUCENE-1101 I don t know if all of the Tokenizers in lucene were ever changed but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it s reusableToken... and now it doesn t. I don t know if all of the Tokenizers in lucene were ever changed but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it s reusableToken... and now it doesn t. No one is calling clearAttributes in trunk code only some of them clear attributes before filling data in. OK I open another issue later and change all Tokenizers in core and contrib to call clearAttributes as first call inside incrementToken But in principle we could also change the indexer to call clear before each incrementToken removing the need to do it in every Tokenizer. But in principle we could also change the indexer to call clear before each incrementToken removing the need to do it in every Tokenizer. Doron brought up a good reason for not doing that in LUCENE-1101. A tokenizer or other token producer could produce multiple tokens before one made it to the ultimate consumer because of stop filters etc . So it looks like producers should do the clear. I opened LUCENE-1801 for that. A patch is available and will be committed soon.
